{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f802b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this script, we will see how to define a custom model and train it using the Mammoth library.\n",
    "\n",
    "In addition to the usual `train` and `load_runner` functions, we will need:\n",
    "- `register_model`: to register our custom model with the Mammoth library.\n",
    "- `ContinualModel`: to define our custom model.\n",
    "\"\"\"\n",
    "\n",
    "from mammoth import register_model, ContinualModel, load_runner, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f722375",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_model('new-sgd') # Register this model with the name 'new-sgd'\n",
    "class NewSgd(ContinualModel):\n",
    "    \"\"\"\n",
    "    Each model must inherit from ContinualModel and implement the observe method.\n",
    "\n",
    "    The observe method is called every time a new batch of data is available.\n",
    "    It is responsible for training the model on the current task using the data provided.\n",
    "\n",
    "    The model can also include a `COMPATIBILITY` attribute to specify the scenarios it is compatible with.\n",
    "\n",
    "    The ContinualModel class provides attrubutes such as:\n",
    "    - `net`: the backbone model that is used for training. The backbone is defined by default in the dataset but can be changed with the `backbone` argument.\n",
    "    - `opt`: the optimizer used for training.\n",
    "    - `loss`: the loss function used for training. This is defined by the dataset and is usually CrossEntropyLoss.\n",
    "    \"\"\"\n",
    "    COMPATIBILITY = ['class-il', 'task-il', 'domain-il', 'general-continual']\n",
    "\n",
    "    def observe(self, inputs, labels, not_aug_inputs, epoch=None):\n",
    "        \"\"\"\n",
    "        We will implement just the simplest algorithm for Continual Learning: SGD.\n",
    "        With SGD, we simply train the model with the data provided, with no countermeasures against forgetting.\n",
    "        \"\"\"\n",
    "        # zero the gradients\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "        # forward pass on the model\n",
    "        outputs = self.net(inputs)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        self.opt.step()\n",
    "\n",
    "        # return the loss value, for logging purposes\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728e570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 14-Jun-25 17:55:34 - Trying to load default configuration for model new-sgd but no configuration file found in None.\n",
      "[INFO] 14-Jun-25 17:55:34 - Running in a notebook environment. Forcefully setting num_workers=0 to prevent issues with multiprocessing.\n",
      "[WARNING] 14-Jun-25 17:55:35 - Could not get memory from pynvml. Maybe try `pip install --force-reinstall gpustat`. unsupported operand type(s) for +: 'int' and 'NoneType'\n",
      "[INFO] 14-Jun-25 17:55:35 - Using device cuda:0\n",
      "[INFO] 14-Jun-25 17:55:35 - `wandb_entity` and `wandb_project` not set. Disabling wandb.\n",
      "[INFO] 14-Jun-25 17:55:35 - Using backbone: resnet18\n",
      "[INFO] 14-Jun-25 17:55:35 - Using ResNet as backbone\n",
      "[INFO] 14-Jun-25 17:55:35 - Current working directory: /home/lorib/Projects/mammoth.\n",
      "[INFO] 14-Jun-25 17:55:35 - Main process PID: 22052\n",
      "[INFO] 14-Jun-25 17:55:36 - Using 0 workers for the dataloader.\n",
      "[INFO] 14-Jun-25 17:55:36 - Using 0 workers for the dataloader.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad35c6e0c86140c389ee8328d0a47d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 14-Jun-25 17:55:37 - SIGINT received in notebook. Ignoring to prevent kernel crash.\n",
      "[INFO] 14-Jun-25 17:55:38 - Training stopped by signal handler.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we can use the `load_runner` function to load our custom model.\n",
    "\"\"\"\n",
    "\n",
    "model, dataset = load_runner('new-sgd','seq-cifar10',{'lr': 0.1, 'n_epochs': 1, 'batch_size': 32})\n",
    "train(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a8fe2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's create a more sophisticated model that uses a *replay* buffer to store past data and use it to train the model.\n",
    "\n",
    "Replay-based methods (rehearsal) are so common in Continual Learning that Mammoth provides a simple implementation of a replay buffer.\n",
    "To define a buffer, we can use the `Buffer` class, which implements the simple `reservoir sampling` algorithm.\n",
    "In addition, we will use the `add_rehearsal_args` function to add command line arguments for the replay buffer size.\n",
    "\"\"\"\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from mammoth import Buffer, add_rehearsal_args\n",
    "\n",
    "@register_model('experience-replay')  # Register this model with the name 'experience-replay'\n",
    "class ExperienceReplay(ContinualModel):\n",
    "    \"\"\"\n",
    "    This model uses a replay buffer to store past data and use it to train the model.\n",
    "    The replay buffer is defined by the `Buffer` class, which is a simple FIFO queue.\n",
    "    \"\"\"\n",
    "    COMPATIBILITY = ['class-il', 'task-il']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parser(parser: ArgumentParser):\n",
    "        \"\"\"\n",
    "        This method is used to define additional command line arguments for the model.\n",
    "        It is called by the `load_runner` function to parse the arguments.\n",
    "        \"\"\"\n",
    "\n",
    "        # We can add the rehearsal arguments to the parser.\n",
    "        # This includes the `--buffer_size` argument, which defines the size of the replay buffer, and the `--minibatch_size` argument, which defines the size of the mini-batch used for training.\n",
    "        add_rehearsal_args(parser)\n",
    "\n",
    "        # We can also add other arguments specific to the model.\n",
    "        parser.add_argument('--alpha', type=float, default=0.5,\n",
    "                            help='Controls the balance between new and old data.')\n",
    "        \n",
    "        return parser\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # initialize the replay buffer with the size defined in the command line arguments\n",
    "        self.buffer = Buffer(buffer_size=self.args.buffer_size) \n",
    "\n",
    "    def observe(self, inputs, labels, not_aug_inputs, epoch=None):\n",
    "        \"\"\"\n",
    "        We will implement a simple experience replay algorithm.\n",
    "        We will store the data in the replay buffer and use it to train the model.\n",
    "        \"\"\"\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "        # compute the loss on the examples from the current task\n",
    "        outputs = self.net(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        # Sample a batch from the buffer\n",
    "        if len(self.buffer) > 0:\n",
    "            buffer_inputs, buffer_labels = self.buffer.get_data(\n",
    "                size=self.args.minibatch_size, device=self.device)\n",
    "            \n",
    "            # Forward pass on the buffer data\n",
    "            buffer_outputs = self.net(buffer_inputs)\n",
    "            # Compute the loss on the buffer data\n",
    "            buffer_loss = self.loss(buffer_outputs, buffer_labels)\n",
    "            # Combine the losses from the current batch and the buffer\n",
    "            loss = loss + self.args.alpha * buffer_loss\n",
    "\n",
    "        # backward pass and update the weights\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        # Store the current batch in the buffer\n",
    "        self.buffer.add_data(not_aug_inputs, labels)\n",
    "\n",
    "        # return the loss value, for logging purposes\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f62ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 14-Jun-25 17:55:39 - Trying to load default configuration for model experience-replay but no configuration file found in None.\n",
      "[INFO] 14-Jun-25 17:55:39 - Running in a notebook environment. Forcefully setting num_workers=0 to prevent issues with multiprocessing.\n",
      "[INFO] 14-Jun-25 17:55:39 - `wandb_entity` and `wandb_project` not set. Disabling wandb.\n",
      "[INFO] 14-Jun-25 17:55:39 - Using backbone: resnet18\n",
      "[INFO] 14-Jun-25 17:55:39 - Using ResNet as backbone\n",
      "[INFO] 14-Jun-25 17:55:39 - Current working directory: /home/lorib/Projects/mammoth.\n",
      "[INFO] 14-Jun-25 17:55:39 - Main process PID: 22052\n",
      "[INFO] 14-Jun-25 17:55:40 - Using 0 workers for the dataloader.\n",
      "[INFO] 14-Jun-25 17:55:40 - Using 0 workers for the dataloader.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158438f7771d4e228e69ee435c7ca667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 14-Jun-25 17:55:45 - SIGINT received in notebook. Ignoring to prevent kernel crash.\n",
      "[INFO] 14-Jun-25 17:55:45 - Training stopped by signal handler.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now let's see it in action.\n",
    "\"\"\"\n",
    "\n",
    "args = {\n",
    "    # these are the same arguments as before, but we will add the buffer size and minibatch size\n",
    "    'lr': 0.1, \n",
    "    'n_epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    # now we can pass the buffer size and minibatch size as arguments \n",
    "    'buffer_size': 1000, \n",
    "    'minibatch_size': 32, \n",
    "    'alpha': 0.2\n",
    "    }\n",
    "\n",
    "model, dataset = load_runner('experience-replay','seq-cifar10', args)\n",
    "train(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb2993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
