{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfab52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this script we will define a new custom dataset to be used with the Mammoth framework.\n",
    "\n",
    "We will need:\n",
    "- The `register_dataset` function to register our dataset.\n",
    "- The `ContinualDataset` basas class to inherit from.\n",
    "- The `set_default_from_args` function to set default parameters from command line arguments.\n",
    "- The `store_masked_loaders` function to convert the datasets into chunks for continual learning.\n",
    "\n",
    "In addition, we will use the `base_path` function to get the path where the dataset files will be stored,\n",
    "  and the `load_runner` and `train` functions to run our training process.\n",
    "\"\"\"\n",
    "\n",
    "from mammoth import register_dataset, ContinualDataset, load_runner, train, base_path, set_default_from_args, store_masked_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4473dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before defining a Continual Learning dataset to use in Mammoth, we need a data source. \n",
    "In Mammoth this is usually done by craeting a \"joint\" dataset, which is a dataset that contains all the data from all tasks.\n",
    "This dataset will be then split into tasks later on.\n",
    "We will use the CIFAR10 dataset as our data source in this example.\n",
    "\n",
    "The source dataset SHOULD be a subclass of `torch.utils.data.Dataset` (or implement the required `__len__` and `__getitem__` methods).\n",
    "\n",
    "In addition, the dataset MUST define:\n",
    "- `data` and `targets` attributes, which contain the training/testing data and labels respectively.\n",
    "- `not_aug_transform` attribute, which is a transformation that does not apply any data augmentation.\n",
    "- `__getitem__` method, which returns a tuple of (image, label, not_aug_image) where:\n",
    "    - `image` is the transformed image (with data augmentation applied).\n",
    "    - `label` is the label of the image.\n",
    "    - `not_aug_image` is the original image without any data augmentation applied.\n",
    "\n",
    "The `not_aug_image` is used by rehearsal methods to store the original image without any data augmentation applied.\n",
    "The presence of this attribute is also the main reason why we cannot simply use the `torchvision.datasets.CIFAR10` dataset directly, as it returns only the transformed image and label.\n",
    "\"\"\"\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MammothCIFAR10(CIFAR10):\n",
    "    \"\"\"\n",
    "    Overrides the CIFAR10 dataset to change the getitem function.\n",
    "\n",
    "    The CIFAR10 dataset already contains the data and targets attributes, so we do not need to redefine them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, is_train=True, transform=None) -> None:\n",
    "        \"\"\"\n",
    "        Implementing the constructor is not strictly necessary, but it is usually required to load the data and targets in more practical scenarios where data does not simply come from torchvision.\n",
    "        \"\"\"\n",
    "        # the `not self._check_integrity()` is just a trick to avoid printing debug messages\n",
    "        self.root=root\n",
    "        self.not_aug_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super(MammothCIFAR10, self).__init__(root, is_train, transform, download=not self._check_integrity())  \n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Gets the requested element from the dataset.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # In order to apply data augmentation, we need to convert the image from a numpy array to a PIL Image.\n",
    "        img = Image.fromarray(img, mode='RGB')\n",
    "        original_img = img.copy() # if you do not copy the image, the original image will be modified by the data augmentation transformations.\n",
    "\n",
    "        # Apply the not_aug_transform to get the original image without any data augmentation.\n",
    "        not_aug_img = self.not_aug_transform(original_img)\n",
    "\n",
    "        # Apply the transform to get the augmented image.\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target, not_aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_dataset(name='custom-cifar10')\n",
    "class CustomSeqCifar10(ContinualDataset):\n",
    "    \"\"\"\n",
    "    This is the main class that defines a custom Continual Learning dataset in Mammoth.\n",
    "    It MUST inherit from `ContinualDataset` and implement the required attributes and methods.\n",
    "\n",
    "    The required attributes are:\n",
    "    - NAME: name of the dataset.\n",
    "    - SETTING: setting of the dataset ('class-il','domain-il',...).\n",
    "    - SIZE: size of the images in the dataset. This is usually a tuple of (height, width).\n",
    "    - N_CLASSES_PER_TASK: number of classes for each task. It can be a list of integers, where each integer represents the number of classes for each task.\n",
    "    - N_TASKS: number of tasks.\n",
    "    - MEAN: tuple of means for each channel of the dataset.\n",
    "    - STD: tuple of standard deviations for each channel of the dataset.\n",
    "    - TRANSFORM: torchvision transform to apply to the dataset during *training*.\n",
    "    - TEST_TRANSFORM: torchvision transform to apply to the dataset during *testing*.\n",
    "\n",
    "    In addition, it MUST implement the `get_data_loaders` method that returns the train and test datasets and the `get_backbone` method that returns the name of the backbone architecture to use for training.\n",
    "    These datasets will be those that we defined earlier, which inherit from `MammothDataset` and implement the required methods.\n",
    "    The `get_data_loaders` almast always end with a call to `store_masked_loaders`, which will convert the datasets into chunks for continual learning.\n",
    "    \"\"\"\n",
    "\n",
    "    NAME = 'custom-cifar10'\n",
    "    SETTING = 'class-il'\n",
    "    SIZE = (32, 32)\n",
    "    N_CLASSES_PER_TASK = 2\n",
    "    N_TASKS = 5\n",
    "    MEAN, STD = (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2615)\n",
    "    TRANSFORM = transforms.Compose(\n",
    "        [transforms.RandomCrop(32, padding=4),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(MEAN, STD)])\n",
    "    TEST_TRANSFORM = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STD)])\n",
    "\n",
    "    def get_data_loaders(self):\n",
    "        \"\"\"\n",
    "        Class method that returns the train and test loaders.\n",
    "        \"\"\"\n",
    "        train_dataset = MammothCIFAR10(base_path() + 'CIFAR10', is_train=True, transform=self.TRANSFORM)\n",
    "        test_dataset = MammothCIFAR10(base_path() + 'CIFAR10', is_train=False, transform=self.TEST_TRANSFORM)\n",
    "\n",
    "        return store_masked_loaders(train_dataset, test_dataset, self)\n",
    "\n",
    "    @set_default_from_args(\"backbone\")\n",
    "    def get_backbone():\n",
    "        \"\"\"\n",
    "        The name of a registered backbone (see `create_a_backbone.ipynb` for more details).  \n",
    "        \"\"\"\n",
    "        return \"resnet18\"\n",
    "\n",
    "    def get_loss(self):\n",
    "        return CrossEntropyLoss()\n",
    "\n",
    "    def get_transform(self):\n",
    "        return self.TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d23865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can use the `load_runner` function to load our model on the custom dataset.\n",
    "\"\"\"\n",
    "\n",
    "model, dataset = load_runner('sgd','custom-cifar10',{'lr': 0.1, 'n_epochs': 1, 'batch_size': 32})\n",
    "train(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a857877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
